#!/usr/bin/env python3
"""
å¾®ä¿¡å…¬ä¼—å·é¢„æŠ“å–è„šæœ¬
ç›´æ¥è¯»å–æœ¬åœ° we-mp-rss SQLite æ•°æ®åº“ï¼Œè¾“å‡º JSON åˆ° stdout

ç”¨æ³•:
  python3 fetch_wechat.py YYYY-MM-DD [--log]

æ—¶é—´çª—å£: å‰ä¸€å¤© 08:00 CST â†’ å½“å¤© 08:00 CST
è¾“å‡º: JSON åˆ° stdoutï¼ˆå« title, link, mp_name, summary, contentï¼‰
"""
import json
import re
import sqlite3
import sys
import os
from datetime import datetime, timedelta, timezone

# åŒ—äº¬æ—¶é—´ = UTC+8
CST = timezone(timedelta(hours=8))

DB_PATH = os.path.expanduser(
    "~/Library/CloudStorage/Dropbox/CC/Projects/we-mp-rss/data/db.db"
)

# ===== PR/è½¯æ–‡è¿‡æ»¤ =====

# æ ‡é¢˜ä¸­å‡ºç°è¿™äº›å…³é”®è¯ â†’ å¤§æ¦‚ç‡æ˜¯ PR è½¯æ–‡æˆ–ä½ä»·å€¼å†…å®¹ï¼Œç›´æ¥è¿‡æ»¤
TITLE_BLACKLIST = [
    r'é€\s*\d+\s*æ¡æç¤ºè¯',    # "é€ 200 æ¡æç¤ºè¯ï¼"
    r'å°ç™½ä¹Ÿèƒ½',               # æ•™ç¨‹å¼•æµ
    r'æç®€æ•™ç¨‹',               # æ•™ç¨‹å¼•æµ
    r'æ‰‹æŠŠæ‰‹æ•™ä½ ',             # æ•™ç¨‹å¼•æµ
    r'å…è´¹é¢†å–',
    r'é™æ—¶å…è´¹',
    r'ä¼˜æƒ åˆ¸',
    r'æŠ˜æ‰£ç ',
    r'æŠ½å¥–',
    r'è½¬å‘.*æŠ½',
    r'èµ é€.*åé¢',
    r'æ‰«ç .*é¢†',
    r'åŠ ç¾¤',
    r'æ˜Ÿçƒ',                   # çŸ¥è¯†æ˜Ÿçƒå¼•æµ
    r'çŸ¥è¯†æ˜Ÿçƒ',
]

# æ­£æ–‡ä¸­å‡ºç°è¿™äº›ç‰¹å¾ â†’ æ ‡è®°ä¸ºç–‘ä¼¼ PRï¼ˆä¸ç›´æ¥è¿‡æ»¤ï¼Œæ ‡è®°è®© LLM åˆ¤æ–­ï¼‰
CONTENT_PR_SIGNALS = [
    r'æœ¬æ–‡ç”±.*æä¾›',
    r'æœ¬æ–‡ç³».*æŠ•ç¨¿',
    r'å¹¿å‘Š|æ¨å¹¿|èµåŠ©',
    r'ç‚¹å‡»é˜…è¯»åŸæ–‡.*è´­ä¹°',
    r'æ‰«ç .*è´­ä¹°',
    r'ä¼˜æƒ ä»·|é™æ—¶ç‰¹ä»·|ç«‹å³æŠ¢è´­',
    r'å“ç‰Œåˆä½œ|å•†åŠ¡åˆä½œ',
]


def is_title_blacklisted(title):
    """æ ‡é¢˜å‘½ä¸­é»‘åå• â†’ ç›´æ¥è¿‡æ»¤"""
    for pattern in TITLE_BLACKLIST:
        if re.search(pattern, title):
            return True
    return False


def count_pr_signals(content):
    """ç»Ÿè®¡æ­£æ–‡ä¸­ PR ä¿¡å·æ•°é‡"""
    if not content:
        return 0
    count = 0
    for pattern in CONTENT_PR_SIGNALS:
        if re.search(pattern, content[:3000]):  # åªæ£€æŸ¥å‰ 3000 å­—
            count += 1
    return count


def is_short_fragment(title, content):
    """æ£€æµ‹æœ‹å‹åœˆå¼ç¢ç‰‡å†…å®¹ï¼ˆtitle å¾ˆé•¿åƒæ­£æ–‡ã€æ²¡æœ‰å®é™… contentï¼‰"""
    # title è¶…è¿‡ 80 å­—ä¸”æ²¡æœ‰å…¨æ–‡ â†’ å¤§æ¦‚ç‡æ˜¯æœ‹å‹åœˆ/è½¬å‘ç¢ç‰‡
    if len(title) > 80 and (not content or len(content) < 100):
        return True
    return False


def main():
    if len(sys.argv) < 2:
        print("Usage: python3 fetch_wechat.py YYYY-MM-DD [--log]", file=sys.stderr)
        sys.exit(1)

    date_str = sys.argv[1]
    do_log = '--log' in sys.argv

    base_dir = "/Users/justin/Library/CloudStorage/Dropbox/CC/Projects/Daily News"
    log_dir = os.path.join(base_dir, "fetch_log")

    if not os.path.exists(DB_PATH):
        print(f"âŒ æ•°æ®åº“ä¸å­˜åœ¨: {DB_PATH}", file=sys.stderr)
        sys.exit(1)

    # æ—¶é—´çª—å£: å‰ä¸€å¤© 08:00 CST â†’ å½“å¤© 08:00 CST
    target = datetime.strptime(date_str, '%Y-%m-%d')
    start_cst = datetime(target.year, target.month, target.day, 8, 0, 0, tzinfo=CST) - timedelta(days=1)
    end_cst = datetime(target.year, target.month, target.day, 8, 0, 0, tzinfo=CST)

    # è½¬ä¸º UTC æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
    start_ts = int(start_cst.timestamp())
    end_ts = int(end_cst.timestamp())

    print(f"ğŸ“… æ—¶é—´çª—å£ (CST): {start_cst.strftime('%Y-%m-%d %H:%M')} â†’ {end_cst.strftime('%Y-%m-%d %H:%M')}", file=sys.stderr)

    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row

    # æŸ¥è¯¢æ–‡ç«  + å…¬ä¼—å·åç§°
    # we-mp-rss è¡¨ç»“æ„:
    #   articles: id, mp_id, title, url, description, content, publish_time, ...
    #   feeds: id, mp_name, ...
    # publish_time æ˜¯ Unix æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
    query = """
        SELECT
            a.title,
            a.url,
            a.content,
            a.description,
            a.publish_time,
            f.mp_name
        FROM articles a
        JOIN feeds f ON a.mp_id = f.id
        WHERE a.publish_time >= ? AND a.publish_time < ?
        ORDER BY a.publish_time DESC
    """

    rows = conn.execute(query, (start_ts, end_ts)).fetchall()
    conn.close()

    articles = []
    filtered_count = 0
    for row in rows:
        title = (row['title'] or '').strip()
        url = (row['url'] or '').strip()
        content = (row['content'] or '').strip()
        description = (row['description'] or '').strip()
        mp_name = (row['mp_name'] or '').strip()
        publish_time = row['publish_time']

        # === è¿‡æ»¤å±‚ ===

        # 1. æ ‡é¢˜é»‘åå• â†’ ç›´æ¥è·³è¿‡
        if is_title_blacklisted(title):
            print(f"  â›” æ ‡é¢˜è¿‡æ»¤: [{mp_name}] {title[:50]}", file=sys.stderr)
            filtered_count += 1
            continue

        # 2. æœ‹å‹åœˆç¢ç‰‡å†…å®¹ â†’ ç›´æ¥è·³è¿‡
        if is_short_fragment(title, content):
            print(f"  â›” ç¢ç‰‡è¿‡æ»¤: [{mp_name}] {title[:50]}", file=sys.stderr)
            filtered_count += 1
            continue

        # æ ¼å¼åŒ–æ—¶é—´
        if publish_time:
            dt = datetime.fromtimestamp(publish_time, tz=CST)
            pub_str = dt.strftime('%Y-%m-%d %H:%M CST')
        else:
            pub_str = ''

        has_content = bool(content and len(content) > 100)

        # 3. PR ä¿¡å·æ£€æµ‹ â†’ æ ‡è®°ä½†ä¸è¿‡æ»¤ï¼ˆç•™ç»™ LLM æœ€ç»ˆåˆ¤æ–­ï¼‰
        pr_signals = count_pr_signals(content)

        # summary ä¼˜å…ˆç”¨å…¨æ–‡æˆªå–ï¼Œæ— å…¨æ–‡åˆ™ç”¨ description
        summary_text = content[:500] if has_content else description[:500]

        article = {
            'title': title,
            'link': url,
            'mp_name': mp_name,
            'summary': summary_text,
            'has_full_content': has_content,
            'content_length': len(content) if content else 0,
            'publish_time': pub_str,
        }

        # æœ‰ PR ä¿¡å·æ—¶æ ‡è®°ï¼Œæé†’ LLM æ³¨æ„
        if pr_signals > 0:
            article['pr_warning'] = f"æ£€æµ‹åˆ° {pr_signals} ä¸ªç–‘ä¼¼ PR/æ¨å¹¿ä¿¡å·"

        articles.append(article)

    total = len(rows)
    kept = len(articles)
    print(f"âœ… {kept} ç¯‡æ–‡ç« ï¼ˆè¿‡æ»¤ {filtered_count} ç¯‡ï¼Œ{sum(1 for a in articles if a['has_full_content'])} ç¯‡æœ‰å…¨æ–‡ï¼‰", file=sys.stderr)
    if filtered_count > 0:
        print(f"  ğŸ“‹ è¿‡æ»¤åŸå› : æ ‡é¢˜é»‘åå•/æœ‹å‹åœˆç¢ç‰‡", file=sys.stderr)

    # è¾“å‡º JSON
    print(json.dumps(articles, ensure_ascii=False, indent=2))

    # å†™æ—¥å¿—
    if do_log:
        os.makedirs(log_dir, exist_ok=True)
        log_file = os.path.join(log_dir, f"{date_str}-wechat.json")
        log_data = {
            'date': date_str,
            'window_cst': f"{start_cst.strftime('%Y-%m-%d %H:%M')} â†’ {end_cst.strftime('%Y-%m-%d %H:%M')}",
            'total_articles': len(articles),
            'with_content': sum(1 for a in articles if a['has_full_content']),
            'article_links': [a['link'] for a in articles],
        }
        with open(log_file, 'w') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ æ—¥å¿—å·²å†™å…¥ {log_file}", file=sys.stderr)


if __name__ == '__main__':
    main()
