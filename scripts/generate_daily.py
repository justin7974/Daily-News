#!/usr/bin/env python3
"""
ç”ŸæˆDaily Newsæ—¥æŠ¥ - ä¼˜åŒ–ç‰ˆ
"""

import feedparser
import json
import subprocess
import sys
from datetime import datetime, timedelta
import os
import re

def load_json(path):
    with open(path, 'r') as f:
        return json.load(f)

def parse_date(date_str):
    if not date_str:
        return None
    formats = [
        '%a, %d %b %Y %H:%M:%S %z',
        '%a, %d %b %Y %H:%M:%S %Z',
        '%Y-%m-%dT%H:%M:%S%z',
        '%Y-%m-%dT%H:%M:%SZ',
    ]
    for fmt in formats:
        try:
            return datetime.strptime(date_str, fmt)
        except:
            continue
    return None

def fetch_rss_articles(config_path, target_date):
    """æŠ“å–RSSæ–‡ç« """
    sources = load_json(config_path)
    start_time = target_date.replace(hour=8, minute=0, second=0, microsecond=0)
    end_time = start_time + timedelta(days=1)
    
    articles = []
    for source in sources[:20]:  # é™åˆ¶æºæ•°é‡
        try:
            feed = feedparser.parse(source['url'])
            if not hasattr(feed, 'entries'):
                continue
            
            for entry in feed.entries[:5]:  # æ¯ä¸ªæºæœ€å¤š5ç¯‡
                published = parse_date(entry.get('published') or entry.get('pubDate') or entry.get('updated'))
                if published:
                    if published.tzinfo:
                        published = published.replace(tzinfo=None)
                    if start_time <= published < end_time:
                        articles.append({
                            'title': entry.get('title', 'æ— æ ‡é¢˜'),
                            'link': entry.get('link', ''),
                            'summary': (entry.get('summary') or entry.get('description') or '')[:400],
                            'source': source['name'],
                            'category': source.get('category', ''),
                        })
        except Exception as e:
            pass
    
    return articles

def fetch_twitter_batch(handles, target_date):
    """æ‰¹é‡æŠ“å–Twitter"""
    start_time = target_date.replace(hour=8, minute=0, second=0, microsecond=0)
    end_time = start_time + timedelta(days=1)
    
    all_tweets = []
    for handle in handles:
        try:
            result = subprocess.run(
                ['bird', 'search', f'from:{handle}', '-n', '5', '--plain'],
                capture_output=True, text=True, timeout=20
            )
            if result.returncode != 0:
                continue
            
            lines = result.stdout.split('\n')
            current = {}
            for line in lines:
                line = line.strip()
                if not line or line.startswith('â”€'):
                    if current.get('text') and 'date' in current:
                        try:
                            tweet_date = datetime.strptime(current['date'], '%a %b %d %H:%M:%S %z %Y')
                            tweet_date = tweet_date.replace(tzinfo=None)
                            if start_time <= tweet_date < end_time:
                                current['handle'] = handle
                                all_tweets.append(current)
                        except:
                            pass
                    current = {}
                    continue
                
                if line.startswith('date:'):
                    current['date'] = line[5:].strip()
                elif line.startswith('url:'):
                    current['url'] = line[4:].strip()
                elif line and not line.startswith('@') and not line.startswith('>'):
                    current['text'] = line[:350]
        except:
            pass
    
    return all_tweets

def generate_markdown(date_str, articles, tweets):
    lines = [f"# Daily News | {date_str}", ""]
    
    # RSS
    lines.extend(["## RSS æ—¥æŠ¥", ""])
    
    # é‡ç‚¹æ¨è
    featured = articles[:6] if len(articles) >= 6 else articles
    if featured:
        lines.extend(["### ğŸ”¥ é‡ç‚¹æ¨è", ""])
        for i, a in enumerate(featured, 1):
            lines.append(f"**{i}. {a['title']}** ({a['source']})")
            lines.append(a['link'])
            lines.append("")
            if a.get('summary'):
                summary = a['summary'].replace('\n', ' ')[:250]
                lines.append(f"{summary}...")
                lines.append("")
            lines.append("ğŸ¦ç‚¹è¯„ï¼šå€¼å¾—å…³æ³¨çš„æŠ€æœ¯åŠ¨æ€")
            lines.append("")
    
    # å…¶ä»–æ–°é—»
    others = articles[6:12]
    if others:
        lines.extend(["### ğŸ“Œ å…¶ä»–æ–°é—»", ""])
        for a in others:
            lines.append(f"**{a['title']}** ({a['source']})")
            lines.append(a['link'])
            lines.append("")
    
    # Twitter
    lines.extend(["## Twitter KOL æ—¥æŠ¥", ""])
    
    # åˆ†ç±»æ¨æ–‡
    ai_handles = ['karpathy', 'emollick', 'Hesamation', 'vasuman', 'EXM7777', 'kloss_xyz']
    startup_handles = ['gregisenberg', 'levelsio', 'marclou', 'MengTo', 'rileybrown']
    
    ai_tweets = [t for t in tweets if t['handle'] in ai_handles][:5]
    startup_tweets = [t for t in tweets if t['handle'] in startup_handles][:5]
    other_tweets = [t for t in tweets if t['handle'] not in ai_handles + startup_handles][:5]
    
    if ai_tweets:
        lines.extend(["### ğŸ§  AI æŠ€æœ¯å‰æ²¿", ""])
        for t in ai_tweets:
            lines.append(f"**@{t['handle']}**")
            lines.append(f"> {t.get('text', '')}")
            if t.get('url'):
                lines.append(f"ğŸ”— {t['url']}")
            lines.append("")
    
    if startup_tweets:
        lines.extend(["### ğŸš€ åˆ›ä¸šåŠ¨æ€", ""])
        for t in startup_tweets:
            lines.append(f"**@{t['handle']}**")
            lines.append(f"> {t.get('text', '')}")
            if t.get('url'):
                lines.append(f"ğŸ”— {t['url']}")
            lines.append("")
    
    if other_tweets:
        lines.extend(["### ğŸ’¬ è§‚ç‚¹ä¸æ´å¯Ÿ", ""])
        for t in other_tweets:
            lines.append(f"**@{t['handle']}**")
            lines.append(f"> {t.get('text', '')}")
            if t.get('url'):
                lines.append(f"ğŸ”— {t['url']}")
            lines.append("")
    
    lines.append("*Generated by å°è™¾ ğŸ¦*")
    return '\n'.join(lines)

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: python3 generate_daily.py YYYY-MM-DD")
        sys.exit(1)
    
    date_str = sys.argv[1]
    target_date = datetime.strptime(date_str, '%Y-%m-%d')
    
    base_dir = "/Users/justin/Library/CloudStorage/Dropbox/CC/Projects/Daily News"
    
    print(f"ğŸ“… ç”Ÿæˆ {date_str} æ—¥æŠ¥...", file=sys.stderr)
    
    # æŠ“å–RSS
    print("ğŸ”„ RSS...", file=sys.stderr)
    articles = fetch_rss_articles(f"{base_dir}/config/rss_sources.json", target_date)
    print(f"   {len(articles)}ç¯‡", file=sys.stderr)
    
    # æŠ“å–Twitter
    print("ğŸ”„ Twitter...", file=sys.stderr)
    kols = load_json(f"{base_dir}/config/twitter_kols.json")
    all_handles = []
    for group in kols.get('groups', {}).values():
        all_handles.extend(group)
    tweets = fetch_twitter_batch(all_handles, target_date)
    print(f"   {len(tweets)}æ¡", file=sys.stderr)
    
    # ç”Ÿæˆ
    print("ğŸ“ Markdown...", file=sys.stderr)
    md = generate_markdown(date_str, articles, tweets)
    
    output_path = f"{base_dir}/content/{date_str}.md"
    with open(output_path, 'w') as f:
        f.write(md)
    
    print(f"âœ… {output_path}", file=sys.stderr)
    print(json.dumps({"articles": len(articles), "tweets": len(tweets)}))
