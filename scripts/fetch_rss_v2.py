#!/usr/bin/env python3
"""
RSS é¢„æŠ“å–è„šæœ¬ v2
108 æºå…¨é‡å¹¶å‘æŠ“å– + è§„åˆ™è¿‡æ»¤ + tier æ’åº

ç”¨æ³•:
  python3 fetch_rss_v2.py YYYY-MM-DD [--log]

æ—¶é—´çª—å£: å‰ä¸€å¤© 08:00 CST â†’ å½“å¤© 08:00 CSTï¼ˆå³ å‰ä¸€å¤© 00:00 UTC â†’ å½“å¤© 00:00 UTCï¼‰
è¾“å‡º: JSON åˆ° stdout
"""
import feedparser
import json
import sys
import os
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
from urllib.parse import urlparse

# åŒ—äº¬æ—¶é—´ = UTC+8
CST = timezone(timedelta(hours=8))

CHINA_AI_KEYWORDS = [
    'AI', 'äººå·¥æ™ºèƒ½', 'å¤§æ¨¡å‹', 'LLM', 'GPT', 'Claude', 'Gemini',
    'DeepSeek', 'èŠ¯ç‰‡', 'GPU', 'SaaS', 'æœºå™¨äºº', 'robot',
    'èèµ„', 'æŠ•èµ„', 'ä¼°å€¼', 'VC', 'åˆ›ä¸š', 'ç‹¬è§’å…½',
    'AGI', 'Agent', 'è‡ªåŠ¨é©¾é©¶', 'ç”Ÿæˆå¼',
]


def parse_entry_date(entry):
    """è§£æ RSS entry çš„å‘å¸ƒæ—¶é—´ï¼Œè¿”å› aware UTC datetime"""
    raw = (entry.get('published') or entry.get('updated')
           or entry.get('pubDate') or entry.get('created'))
    if not raw:
        return None

    # feedparser å·²ç»è§£æå¥½çš„ struct_time
    for field in ('published_parsed', 'updated_parsed'):
        parsed = entry.get(field)
        if parsed:
            try:
                from calendar import timegm
                ts = timegm(parsed)
                return datetime.fromtimestamp(ts, tz=timezone.utc)
            except Exception:
                pass

    # æ‰‹åŠ¨è§£æå¸¸è§æ ¼å¼
    formats = [
        '%a, %d %b %Y %H:%M:%S %z',
        '%Y-%m-%dT%H:%M:%S%z',
        '%Y-%m-%dT%H:%M:%SZ',
        '%Y-%m-%d %H:%M:%S%z',
        '%Y-%m-%d %H:%M:%S',
    ]
    for fmt in formats:
        try:
            s = raw.strip()
            if fmt.endswith('Z'):
                s = s.replace('+00:00', 'Z').replace('+0000', 'Z')
            dt = datetime.strptime(s, fmt)
            if dt.tzinfo is None:
                # æ— æ—¶åŒºä¿¡æ¯çš„å‡è®¾ä¸º UTC
                dt = dt.replace(tzinfo=timezone.utc)
            else:
                dt = dt.astimezone(timezone.utc)
            return dt
        except (ValueError, OverflowError):
            continue
    return None


def needs_keyword_filter(source_name):
    """China-AI/VC æºéœ€è¦å…³é”®è¯è¿‡æ»¤"""
    china_sources = ['36kr', '36æ°ª', 'é‡å­ä½', 'è™å—…', 'InfoQ', 'é›·é”‹ç½‘']
    return any(k.lower() in source_name.lower() for k in china_sources)


def matches_keywords(title, summary):
    text = (title + ' ' + summary).lower()
    return any(kw.lower() in text for kw in CHINA_AI_KEYWORDS)


def fetch_single_feed(args):
    """æŠ“å–å•ä¸ª RSS æºï¼Œè¿”å›æ—¶é—´çª—å£å†…çš„æ–‡ç« åˆ—è¡¨"""
    name, url, category, tier, start_utc, end_utc = args
    try:
        feed = feedparser.parse(url)
        articles = []
        for entry in feed.entries[:15]:
            dt = parse_entry_date(entry)
            if not dt:
                continue
            if not (start_utc <= dt < end_utc):
                continue

            title = entry.get('title', 'æ— æ ‡é¢˜').strip()
            summary = (entry.get('summary') or entry.get('description') or '')[:500]
            link = entry.get('link', '')

            # China æºå…³é”®è¯è¿‡æ»¤
            if needs_keyword_filter(name) and not matches_keywords(title, summary):
                continue

            articles.append({
                'title': title,
                'link': link,
                'summary': summary,
                'source': name,
                'category': category,
                'tier': tier,
                'date': dt.strftime('%Y-%m-%d %H:%M UTC'),
            })
        return articles
    except Exception as e:
        print(f"âš ï¸ {name}: {e}", file=sys.stderr)
        return []


def load_previous_links(log_dir, target_date_str):
    """è¯»å–å‰ä¸€å¤© fetch logï¼Œæ’é™¤é‡å¤æ–‡ç« """
    prev_date = datetime.strptime(target_date_str, '%Y-%m-%d') - timedelta(days=1)
    prev_file = os.path.join(log_dir, f"{prev_date.strftime('%Y-%m-%d')}.json")
    if not os.path.exists(prev_file):
        return set()
    try:
        with open(prev_file) as f:
            data = json.load(f)
        return set(data.get('article_links', []))
    except Exception:
        return set()


def main():
    if len(sys.argv) < 2:
        print("Usage: python3 fetch_rss_v2.py YYYY-MM-DD [--log]", file=sys.stderr)
        sys.exit(1)

    date_str = sys.argv[1]
    do_log = '--log' in sys.argv

    base_dir = "/Users/justin/Library/CloudStorage/Dropbox/CC/Projects/Daily News"
    config_path = os.path.join(base_dir, "config/rss_sources.json")
    log_dir = os.path.join(base_dir, "fetch_log")

    with open(config_path) as f:
        sources = json.load(f)

    # æ—¶é—´çª—å£: å‰ä¸€å¤© 08:00 CST â†’ å½“å¤© 08:00 CST
    # = å‰ä¸€å¤© 00:00 UTC â†’ å½“å¤© 00:00 UTC
    target = datetime.strptime(date_str, '%Y-%m-%d')
    start_utc = datetime(target.year, target.month, target.day, 0, 0, 0, tzinfo=timezone.utc) - timedelta(days=1)
    end_utc = datetime(target.year, target.month, target.day, 0, 0, 0, tzinfo=timezone.utc)

    print(f"ğŸ“… æ—¶é—´çª—å£ (UTC): {start_utc.strftime('%Y-%m-%d %H:%M')} â†’ {end_utc.strftime('%Y-%m-%d %H:%M')}", file=sys.stderr)
    print(f"ğŸ“… æ—¶é—´çª—å£ (CST): {(start_utc+timedelta(hours=8)).strftime('%Y-%m-%d %H:%M')} â†’ {(end_utc+timedelta(hours=8)).strftime('%Y-%m-%d %H:%M')}", file=sys.stderr)
    print(f"ğŸ“¡ æŠ“å– {len(sources)} ä¸ªæº...", file=sys.stderr)

    # è·¨æ—¥å»é‡
    prev_links = load_previous_links(log_dir, date_str)

    # å¹¶å‘æŠ“å–
    args_list = [
        (s['name'], s['url'], s.get('category', ''), s.get('tier', 3), start_utc, end_utc)
        for s in sources
    ]

    all_articles = []
    with ThreadPoolExecutor(max_workers=20) as executor:
        futures = {executor.submit(fetch_single_feed, a): a[0] for a in args_list}
        for future in as_completed(futures):
            result = future.result()
            all_articles.extend(result)

    # å»é‡: URL å»é‡ + è·¨æ—¥å»é‡
    seen = set()
    deduped = []
    for a in all_articles:
        link = a['link']
        if link in seen or link in prev_links:
            continue
        seen.add(link)
        deduped.append(a)

    # åŒåŸŸåé™åˆ¶ â‰¤ 5 ç¯‡
    domain_count = defaultdict(int)
    filtered = []
    for a in deduped:
        domain = urlparse(a['link']).netloc
        if domain_count[domain] >= 5:
            continue
        domain_count[domain] += 1
        filtered.append(a)

    # æŒ‰ tier æ’åº (tier 1 ä¼˜å…ˆ)
    filtered.sort(key=lambda x: (x.get('tier', 3), x['source']))

    print(f"âœ… {len(filtered)} ç¯‡å€™é€‰æ–‡ç« ï¼ˆå»é‡å‰ {len(all_articles)}ï¼Œè·¨æ—¥æ’é™¤ {len(prev_links)} é“¾æ¥ï¼‰", file=sys.stderr)

    # è¾“å‡º JSON
    print(json.dumps(filtered, ensure_ascii=False, indent=2))

    # å†™æ—¥å¿—
    if do_log:
        os.makedirs(log_dir, exist_ok=True)
        log_file = os.path.join(log_dir, f"{date_str}.json")
        log_data = {
            'date': date_str,
            'window_utc': f"{start_utc.isoformat()} â†’ {end_utc.isoformat()}",
            'total_fetched': len(all_articles),
            'after_dedup': len(filtered),
            'article_links': [a['link'] for a in filtered],
        }
        with open(log_file, 'w') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ æ—¥å¿—å·²å†™å…¥ {log_file}", file=sys.stderr)


if __name__ == '__main__':
    main()
