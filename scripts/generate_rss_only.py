#!/usr/bin/env python3
"""
ä»…ç”ŸæˆRSSæ—¥æŠ¥ï¼ˆTwitterç¨åå•ç‹¬å¤„ç†ï¼‰
"""
import feedparser
import json
import sys
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor

def fetch_single_feed(args):
    name, url, category, start_time, end_time = args
    try:
        feed = feedparser.parse(url)
        articles = []
        for entry in feed.entries[:10]:
            pub = entry.get('published') or entry.get('updated') or entry.get('pubDate')
            if pub:
                for fmt in ['%Y-%m-%dT%H:%M:%S%z', '%a, %d %b %Y %H:%M:%S %z', '%Y-%m-%dT%H:%M:%SZ']:
                    try:
                        dt = datetime.strptime(pub.replace('Z', '+0000'), fmt)
                        dt = dt.replace(tzinfo=None)
                        if start_time <= dt < end_time:
                            articles.append({
                                'title': entry.get('title', 'æ— æ ‡é¢˜'),
                                'link': entry.get('link', ''),
                                'summary': (entry.get('summary') or entry.get('description') or '')[:400],
                                'source': name,
                                'date': dt.strftime('%Y-%m-%d %H:%M'),
                                'category': category
                            })
                        break
                    except:
                        continue
        return articles
    except:
        return []

def fetch_rss(config_path, target_date):
    with open(config_path) as f:
        sources = json.load(f)
    
    start_time = target_date.replace(hour=8, minute=0, second=0, microsecond=0)
    end_time = start_time + timedelta(days=1)
    
    priority = [s for s in sources if s.get('category') in ['AI', 'Startup', 'VC']][:15]
    others = [s for s in sources if s not in priority][:15]
    selected = priority + others
    
    args_list = [(s['name'], s['url'], s.get('category', ''), start_time, end_time) for s in selected]
    
    all_articles = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(fetch_single_feed, args) for args in args_list]
        for future in futures:
            all_articles.extend(future.result())
    
    return sorted(all_articles, key=lambda x: x.get('category', '') in ['AI', 'Startup', 'VC'], reverse=True)

def generate_rss_markdown(date_str, articles):
    lines = [f"# Daily News | {date_str}", ""]
    lines.extend(["## RSS æ—¥æŠ¥", ""])
    
    # é‡ç‚¹æ¨èï¼ˆè‡³å°‘5ç¯‡ï¼‰
    ai_startup = [a for a in articles if a.get('category') in ['AI', 'Startup', 'VC']]
    other = [a for a in articles if a not in ai_startup]
    featured = (ai_startup + other)[:6]
    
    if featured:
        lines.extend(["### ğŸ”¥ é‡ç‚¹æ¨è", ""])
        for i, a in enumerate(featured, 1):
            lines.append(f"**{i}. {a['title']}** ({a['source']})")
            lines.append(a['link'])
            lines.append("")
            if a.get('summary'):
                summary = a['summary'].replace('\n', ' ')[:280]
                lines.append(f"{summary}...")
                lines.append("")
            lines.append("ğŸ¦ç‚¹è¯„ï¼šå€¼å¾—å…³æ³¨çš„æŠ€æœ¯/è¡Œä¸šåŠ¨æ€")
            lines.append("")
    
    # å…¶ä»–æ–°é—»
    others = [a for a in articles if a not in featured][:8]
    if others:
        lines.extend(["### ğŸ“Œ å…¶ä»–æ–°é—»", ""])
        for a in others:
            lines.append(f"**{a['title']}** ({a['source']})")
            lines.append(a['link'])
            lines.append("")
    
    lines.append("*Generated by å°è™¾ ğŸ¦*")
    return '\n'.join(lines)

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: python3 generate_rss_only.py YYYY-MM-DD")
        sys.exit(1)
    
    date_str = sys.argv[1]
    target = datetime.strptime(date_str, '%Y-%m-%d')
    base = "/Users/justin/Library/CloudStorage/Dropbox/CC/Projects/Daily News"
    
    articles = fetch_rss(f"{base}/config/rss_sources.json", target)
    md = generate_rss_markdown(date_str, articles)
    
    out_path = f"{base}/content/{date_str}.md"
    with open(out_path, 'w') as f:
        f.write(md)
    
    print(json.dumps({"date": date_str, "articles": len(articles), "path": out_path}))
