#!/usr/bin/env python3
"""
å®Œæ•´æ—¥æŠ¥ç”Ÿæˆå™¨ - ä¼˜åŒ–ç‰ˆ
"""
import feedparser
import json
import subprocess
import sys
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import os

def fetch_single_feed(args):
    name, url, category, start_time, end_time = args
    try:
        feed = feedparser.parse(url)
        articles = []
        for entry in feed.entries[:10]:
            pub = entry.get('published') or entry.get('updated') or entry.get('pubDate')
            if pub:
                for fmt in ['%Y-%m-%dT%H:%M:%S%z', '%a, %d %b %Y %H:%M:%S %z', '%Y-%m-%dT%H:%M:%SZ']:
                    try:
                        dt = datetime.strptime(pub.replace('Z', '+0000'), fmt)
                        dt = dt.replace(tzinfo=None)
                        if start_time <= dt < end_time:
                            articles.append({
                                'title': entry.get('title', 'æ— æ ‡é¢˜'),
                                'link': entry.get('link', ''),
                                'summary': (entry.get('summary') or entry.get('description') or '')[:400],
                                'source': name,
                                'date': dt.strftime('%Y-%m-%d %H:%M'),
                                'category': category
                            })
                        break
                    except:
                        continue
        return articles
    except:
        return []

def fetch_rss(config_path, target_date):
    with open(config_path) as f:
        sources = json.load(f)
    
    start_time = target_date.replace(hour=8, minute=0, second=0, microsecond=0)
    end_time = start_time + timedelta(days=1)
    
    # é€‰æ‹©å…³é”®æº
    priority = [s for s in sources if s.get('category') in ['AI', 'Startup', 'VC']][:15]
    others = [s for s in sources if s not in priority][:15]
    selected = priority + others
    
    args_list = [(s['name'], s['url'], s.get('category', ''), start_time, end_time) for s in selected]
    
    all_articles = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(fetch_single_feed, args) for args in args_list]
        for future in futures:
            all_articles.extend(future.result())
    
    return sorted(all_articles, key=lambda x: x.get('category', '') == 'AI', reverse=True)

def fetch_twitter_for_user(handle, target_date):
    start_time = target_date.replace(hour=8, minute=0, second=0, microsecond=0)
    end_time = start_time + timedelta(days=1)
    
    try:
        result = subprocess.run(['bird', 'search', f'from:{handle}', '-n', '5', '--plain'],
                              capture_output=True, text=True, timeout=20)
        if result.returncode != 0:
            return []
        
        tweets = []
        lines = result.stdout.split('\n')
        current = {}
        for line in lines:
            line = line.strip()
            if not line or line.startswith('â”€'):
                if current.get('text') and 'date' in current:
                    try:
                        dt = datetime.strptime(current['date'], '%a %b %d %H:%M:%S %z %Y')
                        dt = dt.replace(tzinfo=None)
                        if start_time <= dt < end_time:
                            tweets.append({
                                'handle': handle,
                                'text': current['text'][:350],
                                'url': current.get('url', ''),
                                'date': dt.strftime('%m-%d %H:%M')
                            })
                    except:
                        pass
                current = {}
                continue
            if line.startswith('date:'):
                current['date'] = line[5:].strip()
            elif line.startswith('url:'):
                current['url'] = line[4:].strip()
            elif line and not line.startswith('@') and not line.startswith('>'):
                current['text'] = line
        return tweets
    except:
        return []

def fetch_twitter(kols_config, target_date):
    with open(kols_config) as f:
        kols = json.load(f)
    
    all_handles = []
    for group, handles in kols.get('groups', {}).items():
        all_handles.extend(handles)
    
    all_tweets = []
    # ä¸²è¡ŒæŠ“å–Twitterï¼Œé¿å…APIé™åˆ¶
    for handle in all_handles[:15]:  # é™åˆ¶æ•°é‡
        tweets = fetch_twitter_for_user(handle, target_date)
        all_tweets.extend(tweets)
    
    return all_tweets

def generate_markdown(date_str, articles, tweets):
    lines = [f"# Daily News | {date_str}", ""]
    
    # RSSéƒ¨åˆ†
    lines.extend(["## RSS æ—¥æŠ¥", ""])
    
    # é‡ç‚¹æ¨èï¼ˆè‡³å°‘5ç¯‡ï¼‰
    ai_articles = [a for a in articles if a.get('category') in ['AI', 'Startup', 'VC']]
    other_articles = [a for a in articles if a not in ai_articles]
    featured = (ai_articles + other_articles)[:6]
    
    if featured:
        lines.extend(["### ğŸ”¥ é‡ç‚¹æ¨è", ""])
        for i, a in enumerate(featured, 1):
            lines.append(f"**{i}. {a['title']}** ({a['source']})")
            lines.append(a['link'])
            lines.append("")
            if a.get('summary'):
                summary = a['summary'].replace('\n', ' ')[:280]
                lines.append(f"{summary}...")
                lines.append("")
            lines.append("ğŸ¦ç‚¹è¯„ï¼šå€¼å¾—å…³æ³¨çš„æŠ€æœ¯/è¡Œä¸šåŠ¨æ€")
            lines.append("")
    
    # å…¶ä»–æ–°é—»
    others = [a for a in articles if a not in featured][:8]
    if others:
        lines.extend(["### ğŸ“Œ å…¶ä»–æ–°é—»", ""])
        for a in others:
            lines.append(f"**{a['title']}** ({a['source']})")
            lines.append(a['link'])
            lines.append("")
    
    # Twitteréƒ¨åˆ†
    lines.extend(["## Twitter KOL æ—¥æŠ¥", ""])
    
    # åˆ†ç±»
    ai_handles = ['karpathy', 'emollick', 'Hesamation', 'vasuman', 'EXM7777', 'kloss_xyz', 'godofprompt']
    startup_handles = ['gregisenberg', 'levelsio', 'marclou', 'MengTo', 'rileybrown', 'corbin_braun', 'jackfriks']
    
    ai_tweets = [t for t in tweets if t['handle'] in ai_handles][:6]
    startup_tweets = [t for t in tweets if t['handle'] in startup_handles][:5]
    insight_tweets = [t for t in tweets if t['handle'] not in ai_handles + startup_handles][:5]
    
    if ai_tweets:
        lines.extend(["### ğŸ§  AI æŠ€æœ¯å‰æ²¿", ""])
        for t in ai_tweets:
            lines.append(f"**@{t['handle']}** ({t.get('date', '')})")
            lines.append(f"> {t['text']}")
            if t.get('url'):
                lines.append(f"ğŸ”— {t['url']}")
            lines.append("")
    
    if startup_tweets:
        lines.extend(["### ğŸš€ åˆ›ä¸šåŠ¨æ€", ""])
        for t in startup_tweets:
            lines.append(f"**@{t['handle']}** ({t.get('date', '')})")
            lines.append(f"> {t['text']}")
            if t.get('url'):
                lines.append(f"ğŸ”— {t['url']}")
            lines.append("")
    
    if insight_tweets:
        lines.extend(["### ğŸ’¬ è§‚ç‚¹ä¸æ´å¯Ÿ", ""])
        for t in insight_tweets:
            lines.append(f"**@{t['handle']}** ({t.get('date', '')})")
            lines.append(f"> {t['text']}")
            if t.get('url'):
                lines.append(f"ğŸ”— {t['url']}")
            lines.append("")
    
    lines.append("*Generated by å°è™¾ ğŸ¦*")
    return '\n'.join(lines)

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: python3 generate.py YYYY-MM-DD")
        sys.exit(1)
    
    date_str = sys.argv[1]
    target = datetime.strptime(date_str, '%Y-%m-%d')
    base = "/Users/justin/Library/CloudStorage/Dropbox/CC/Projects/Daily News"
    
    print(f"ğŸ“… {date_str}: RSS...", file=sys.stderr)
    articles = fetch_rss(f"{base}/config/rss_sources.json", target)
    print(f"   {len(articles)} articles", file=sys.stderr)
    
    print(f"ğŸ“… {date_str}: Twitter...", file=sys.stderr)
    tweets = fetch_twitter(f"{base}/config/twitter_kols.json", target)
    print(f"   {len(tweets)} tweets", file=sys.stderr)
    
    md = generate_markdown(date_str, articles, tweets)
    
    out_path = f"{base}/content/{date_str}.md"
    with open(out_path, 'w') as f:
        f.write(md)
    
    result = {"date": date_str, "articles": len(articles), "tweets": len(tweets), "path": out_path}
    print(json.dumps(result))
