#!/usr/bin/env python3
"""
RSSæŠ“å–è„šæœ¬ - ç”¨äºDaily Newsé¡¹ç›®
æŠ“å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„RSSæ–‡ç« 
"""

import feedparser
import json
import sys
from datetime import datetime, timedelta
import time

def load_rss_sources(config_path):
    with open(config_path, 'r') as f:
        return json.load(f)

def parse_date(date_str):
    """è§£æå„ç§æ—¥æœŸæ ¼å¼"""
    formats = [
        '%a, %d %b %Y %H:%M:%S %z',
        '%a, %d %b %Y %H:%M:%S %Z',
        '%Y-%m-%dT%H:%M:%S%z',
        '%Y-%m-%dT%H:%M:%SZ',
        '%Y-%m-%d %H:%M:%S',
    ]
    for fmt in formats:
        try:
            return datetime.strptime(date_str, fmt)
        except:
            continue
    return None

def fetch_feed(url, name):
    """æŠ“å–å•ä¸ªRSSæº"""
    try:
        import socket
        # è®¾ç½®è¶…æ—¶
        socket.setdefaulttimeout(15)
        feed = feedparser.parse(url, timeout=15)
        if feed.bozo:
            print(f"âš ï¸ {name}: è§£æè­¦å‘Š", file=sys.stderr)
        return feed
    except Exception as e:
        print(f"âŒ {name}: æŠ“å–å¤±è´¥ - {e}", file=sys.stderr)
        return None

def is_in_date_range(entry, start_date, end_date):
    """æ£€æŸ¥æ–‡ç« æ˜¯å¦åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…"""
    published = None
    
    # å°è¯•å„ç§æ—¥æœŸå­—æ®µ
    for field in ['published', 'pubDate', 'updated', 'created']:
        if hasattr(entry, field):
            date_str = getattr(entry, field)
            if date_str:
                published = parse_date(date_str)
                if published:
                    break
    
    if not published:
        return False
    
    # è½¬æ¢ä¸ºæ— æ—¶åŒº aware è¿›è¡Œæ¯”è¾ƒ
    if published.tzinfo:
        published = published.replace(tzinfo=None)
    
    return start_date <= published <= end_date

def fetch_all_rss(config_path, target_date):
    """
    æŠ“å–æ‰€æœ‰RSSæºï¼Œè¿”å›æŒ‡å®šæ—¥æœŸèŒƒå›´çš„æ–‡ç« 
    target_date: datetimeå¯¹è±¡ï¼ŒæŠ“å–å½“å¤©08:00åˆ°æ¬¡æ—¥08:00çš„æ–‡ç« 
    """
    sources = load_rss_sources(config_path)
    
    # è®¡ç®—æ—¶é—´èŒƒå›´ï¼šå½“å¤©08:00åˆ°æ¬¡æ—¥08:00
    start_time = target_date.replace(hour=8, minute=0, second=0, microsecond=0)
    end_time = start_time + timedelta(days=1)
    
    print(f"ğŸ“… æŠ“å–æ—¥æœŸèŒƒå›´: {start_time.strftime('%Y-%m-%d %H:%M')} ~ {end_time.strftime('%Y-%m-%d %H:%M')}")
    
    all_articles = []
    
    for source in sources:
        name = source['name']
        url = source['url']
        category = source.get('category', '')
        
        print(f"ğŸ”„ æ­£åœ¨æŠ“å–: {name}...", file=sys.stderr)
        feed = fetch_feed(url, name)
        
        if not feed or not hasattr(feed, 'entries'):
            print(f"âš ï¸ {name}: æ— å†…å®¹", file=sys.stderr)
            continue
        
        for entry in feed.entries:
            if is_in_date_range(entry, start_time, end_time):
                article = {
                    'title': entry.get('title', 'æ— æ ‡é¢˜'),
                    'link': entry.get('link', ''),
                    'summary': entry.get('summary', entry.get('description', '')),
                    'published': entry.get('published', entry.get('pubDate', '')),
                    'source': name,
                    'category': category,
                    'author': entry.get('author', '')
                }
                all_articles.append(article)
        
        time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    return all_articles

if __name__ == '__main__':
    if len(sys.argv) < 3:
        print("Usage: python3 fetch_rss.py <config_path> <YYYY-MM-DD>")
        sys.exit(1)
    
    config_path = sys.argv[1]
    date_str = sys.argv[2]
    
    try:
        target_date = datetime.strptime(date_str, '%Y-%m-%d')
    except ValueError:
        print(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯: {date_str}")
        sys.exit(1)
    
    articles = fetch_all_rss(config_path, target_date)
    print(json.dumps(articles, ensure_ascii=False, indent=2))
